"""
Docling PDF RAG ç³»çµ± - æ··åˆè¨­å‚™ç‰ˆæœ¬
Embedding ç”¨ CPUï¼ˆç©©å®šï¼‰+ Reranker ç”¨ MPSï¼ˆåŠ é€Ÿï¼‰
æœ€ä½³é…ç½®ï¼šé¿å… OOM + ä¿æŒé€Ÿåº¦
"""

import os
import json
import pickle
import hashlib
import numpy as np
from pathlib import Path
from tqdm import tqdm
import fitz  # PyMuPDF
import torch
from sentence_transformers import SentenceTransformer, CrossEncoder
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import time
import gc

# Docling imports
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions

# ========== é…ç½® ==========
@dataclass
class DoclingRAGConfig:
    """é…ç½®"""
    pdf_path: str = "2025èˆå…‰LED21st(å–®é æ°´å°å¯æœå°‹).pdf"
    
    # åˆ†é è¨­ç½®
    temp_dir: str = "temp_pages"
    keep_temp_files: bool = False
    
    # Docling OCR è¨­ç½®
    enable_ocr: bool = True
    enable_table_structure: bool = True
    
    # å¿«å–è¨­ç½®
    cache_dir: str = "docling_cache"
    force_reparse: bool = False
    
    # Embedding åˆç¯©è¨­ç½®
    enable_embedding_filter: bool = True
    embedding_model: str = "BAAI/bge-m3"
    max_embedding_candidates: int = 50
    
    # Rerank è¨­ç½®
    max_final_pages: int = 25
    reranker_batch_size: int = 8  # MPS batch size
    show_top_scores: int = 30
    
    # Query æ“´å±•
    enable_query_expansion: bool = True
    query_expansion_model: str = "gpt-4o-mini"
    max_keywords: int = 5
    
    # RAG è¨­ç½®
    openai_model: str = "gpt-4o"
    max_response_tokens: int = 10000
    temperature: float = 0.1

# ========== OpenAI ==========
try:
    from openai import OpenAI
    client = OpenAI(api_key="YOUR_OPENAI_API_KEY")
except Exception as e:
    raise RuntimeError(f"OpenAI åˆå§‹åŒ–å¤±æ•—ï¼š{e}")

# ========== æ¨¡å‹ ==========
# æ··åˆè¨­å‚™æ¨¡å¼ï¼šEmbedding CPUï¼ˆé¿å… OOMï¼‰+ Reranker MPSï¼ˆåŠ é€Ÿï¼‰
embedding_device = "cpu"
reranker_device = "mps" if torch.backends.mps.is_available() else "cpu"

print(f"\nâœ“ æ··åˆè¨­å‚™æ¨¡å¼ï¼š")
print(f"  - Embedding: CPU")
print(f"  - Reranker: {reranker_device.upper()}ï¼ˆåŠ é€Ÿï¼‰")

print("\nè¼‰å…¥æ¨¡å‹...")
# Embedding æ¨¡å‹ - ç”¨ CPU é¿å… OOM
embedding_model = SentenceTransformer("BAAI/bge-m3", device="cpu")
print("âœ“ Embedding æ¨¡å‹è¼‰å…¥å®Œæˆ (BAAI/bge-m3 on CPU)")

# Reranker æ¨¡å‹ - ç”¨ MPS åŠ é€Ÿ
reranker = CrossEncoder("BAAI/bge-reranker-v2-m3", device=reranker_device)
print(f"âœ“ Reranker æ¨¡å‹è¼‰å…¥å®Œæˆ (BAAI/bge-reranker-v2-m3 on {reranker_device.upper()})")

# æ¸…ç†é¡¯å­˜
if reranker_device == "mps":
    torch.mps.empty_cache()

# ========== é é¢çµæ§‹ ==========
@dataclass
class PageContent:
    """é é¢å…§å®¹"""
    page_no: int
    content: str
    text_length: int
    source_file: str
    
    def to_dict(self):
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict):
        return cls(**data)

# ========== PDF åˆ†é å·¥å…· ==========
class PDFSplitter:
    """PDF åˆ†é å·¥å…·"""
    
    def __init__(self, temp_dir: str = "temp_pages"):
        self.temp_dir = Path(temp_dir)
        self.temp_dir.mkdir(exist_ok=True)
    
    def split_pdf(self, pdf_path: str) -> List[Tuple[int, str]]:
        """å°‡ PDF åˆ‡æˆå–®é æ–‡ä»¶"""
        
        print(f"\n{'='*80}")
        print(f"ğŸ“„ åˆ†å‰² PDFï¼š{pdf_path}")
        print(f"{'='*80}")
        
        pdf = fitz.open(pdf_path)
        total_pages = len(pdf)
        
        print(f"ç¸½é æ•¸ï¼š{total_pages}")
        
        page_files = []
        
        for page_num in tqdm(range(total_pages), desc="åˆ†é ä¸­"):
            page_no = page_num + 1
            
            single_page_pdf = fitz.open()
            single_page_pdf.insert_pdf(pdf, from_page=page_num, to_page=page_num)
            
            page_file = self.temp_dir / f"page_{page_no:04d}.pdf"
            single_page_pdf.save(str(page_file))
            single_page_pdf.close()
            
            page_files.append((page_no, str(page_file)))
        
        pdf.close()
        
        print(f"âœ“ åˆ†é å®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨ï¼š{self.temp_dir}")
        
        return page_files
    
    def cleanup(self):
        """æ¸…ç†è‡¨æ™‚æ–‡ä»¶"""
        if self.temp_dir.exists():
            import shutil
            shutil.rmtree(self.temp_dir)
            print(f"âœ“ å·²æ¸…ç†è‡¨æ™‚æ–‡ä»¶ï¼š{self.temp_dir}")

# ========== Docling è§£æå™¨ ==========
class DoclingParser:
    """Docling PDF è§£æå™¨ï¼ˆåˆ†é ç‰ˆï¼‰"""
    
    def __init__(self, config: DoclingRAGConfig):
        self.config = config
        self.cache_file = os.path.join(config.cache_dir, "parsed_data.pkl")
        os.makedirs(config.cache_dir, exist_ok=True)
        
        self.splitter = PDFSplitter(config.temp_dir)
        self._init_converter()
    
    def _init_converter(self):
        """åˆå§‹åŒ– Docling è½‰æ›å™¨"""
        print("åˆå§‹åŒ– Docling...")
        
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = self.config.enable_ocr
        pipeline_options.do_table_structure = self.config.enable_table_structure
        
        if self.config.enable_table_structure:
            pipeline_options.table_structure_options.do_cell_matching = True
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=pipeline_options
                )
            }
        )
        
        print(f"âœ“ Docling å·²åˆå§‹åŒ–ï¼ˆOCR: {self.config.enable_ocr}ï¼‰")
    
    def get_pdf_hash(self, pdf_path: str) -> str:
        """è¨ˆç®— PDF hash"""
        with open(pdf_path, 'rb') as f:
            return hashlib.md5(f.read(1024 * 1024)).hexdigest()
    
    def should_reparse(self, pdf_path: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦é‡æ–°è§£æ"""
        if self.config.force_reparse:
            return True
        if not os.path.exists(self.cache_file):
            return True
        
        try:
            with open(self.cache_file, 'rb') as f:
                cached_data = pickle.load(f)
                return cached_data.get('pdf_hash') != self.get_pdf_hash(pdf_path)
        except:
            return True
    
    def parse_pdf(self, pdf_path: str) -> Tuple[List[PageContent], Optional[np.ndarray]]:
        """è§£æ PDFï¼ˆåˆ†é è™•ç†ï¼‰+ è¨ˆç®— embeddings"""
        
        if not self.should_reparse(pdf_path):
            print("ğŸ“‚ è¼‰å…¥å¿«å–...")
            return self.load_cache()
        
        print(f"\n{'='*80}")
        print(f"ğŸ“– è§£æ PDFï¼ˆåˆ†é è™•ç†æ¨¡å¼ï¼‰")
        print(f"{'='*80}")
        
        start = time.time()
        
        page_files = self.splitter.split_pdf(pdf_path)
        
        print(f"\nè™•ç† {len(page_files)} é ...")
        pages_content = self._process_pages(page_files)
        
        if not self.config.keep_temp_files:
            try:
                self.splitter.cleanup()
            except Exception as e:
                print(f"æš«å­˜è³‡æ–™å¤¾åˆªé™¤å¤±æ•—ï¼ˆä¸å½±éŸ¿ä½ çš„è³‡æ–™ï¼‰ï¼š{e}")
        
        print(f"\nâœ“ è§£æå®Œæˆï¼è€—æ™‚ï¼š{time.time() - start:.2f} ç§’")
        self._print_stats(pages_content)
        
        # è¨ˆç®— embeddings
        print(f"\nğŸ“Š è¨ˆç®—é é¢ embeddingsï¼ˆé¦–æ¬¡å»ºç«‹ï¼Œä¹‹å¾Œæœƒå¿«å–ï¼‰...")
        embeddings = self._compute_embeddings(pages_content)
        
        # ä¿å­˜å¿«å–ï¼ˆå« embeddingsï¼‰
        self._save_cache(pages_content, pdf_path, embeddings)
        
        return pages_content, embeddings
    
    def _compute_embeddings(self, pages: List[PageContent]) -> np.ndarray:
        """è¨ˆç®—æ‰€æœ‰é é¢çš„ embeddings"""
        page_texts = [page.content for page in pages]
        
        embeddings = embedding_model.encode(
            page_texts,
            normalize_embeddings=True,
            show_progress_bar=True,
            batch_size=16,
            convert_to_numpy=True
        )
        
        print(f"âœ“ Embeddings è¨ˆç®—å®Œæˆ")
        return embeddings
    
    def _process_pages(self, page_files: List[Tuple[int, str]]) -> List[PageContent]:
        """è™•ç†æ‰€æœ‰é é¢"""
        
        pages = []
        
        for page_no, file_path in tqdm(page_files, desc="Docling è™•ç†"):
            if page_no % 10 == 0:
                gc.collect()
            try:
                content = self._process_single_page(page_no, file_path)
                if content:
                    pages.append(content)
            except Exception as e:
                print(f"\nâš ï¸ é é¢ {page_no} è™•ç†å¤±æ•—ï¼š{e}")
        
        return pages
    
    def _process_single_page(self, page_no: int, file_path: str) -> Optional[PageContent]:
        """è™•ç†å–®é """
        
        result = self.converter.convert(file_path)
        document = result.document
        
        markdown = document.export_to_markdown()
        
        if not markdown.strip():
            return None
        
        return PageContent(
            page_no=page_no,
            content=markdown.strip(),
            text_length=len(markdown.strip()),
            source_file=file_path
        )
    
    def _save_cache(self, pages: List[PageContent], pdf_path: str, embeddings: Optional[np.ndarray] = None):
        """ä¿å­˜å¿«å–ï¼ˆåŒ…å« embeddingsï¼‰"""
        print(f"\nğŸ’¾ ä¿å­˜å¿«å–åˆ°ï¼š{self.cache_file}")
        
        data = {
            'pdf_path': pdf_path,
            'pdf_hash': self.get_pdf_hash(pdf_path),
            'parsed_at': datetime.now().isoformat(),
            'pages': [p.to_dict() for p in pages],
            'embeddings': embeddings  # æ–°å¢ï¼šå„²å­˜ embeddings
        }
        
        with open(self.cache_file, 'wb') as f:
            pickle.dump(data, f)
        
        print("âœ“ å¿«å–å·²ä¿å­˜ï¼ˆå« embeddingsï¼‰")
    
    def load_cache(self) -> Tuple[List[PageContent], Optional[np.ndarray]]:
        """è¼‰å…¥å¿«å–ï¼ˆåŒ…å« embeddingsï¼‰"""
        with open(self.cache_file, 'rb') as f:
            data = pickle.load(f)
        
        pages = [PageContent.from_dict(p) for p in data['pages']]
        embeddings = data.get('embeddings', None)
        
        print(f"âœ“ è¼‰å…¥ {len(pages)} é ")
        if embeddings is not None:
            print(f"âœ“ è¼‰å…¥é è¨ˆç®—çš„ embeddings")
        self._print_stats(pages)
        
        return pages, embeddings
    
    def _print_stats(self, pages: List[PageContent]):
        """æ‰“å°çµ±è¨ˆ"""
        if not pages:
            print("âš ï¸ æ²’æœ‰é é¢ï¼")
            return
        
        total_chars = sum(p.text_length for p in pages)
        avg_chars = total_chars / len(pages)
        
        page_nos = sorted([p.page_no for p in pages])
        missing_pages = []
        if page_nos:
            for i in range(page_nos[0], page_nos[-1] + 1):
                if i not in page_nos:
                    missing_pages.append(i)
        
        print(f"\nğŸ“Š çµ±è¨ˆï¼š")
        print(f"  - ç¸½é æ•¸ï¼š{len(pages)}")
        print(f"  - é ç¢¼ç¯„åœï¼š{page_nos[0]} ~ {page_nos[-1]}")
        if missing_pages:
            print(f"  - âš ï¸ ç¼ºå¤±é ï¼š{missing_pages}")
        else:
            print(f"  - âœ“ é ç¢¼é€£çºŒå®Œæ•´")
        print(f"  - ç¸½å­—æ•¸ï¼š{total_chars:,}")
        print(f"  - å¹³å‡æ¯é ï¼š{avg_chars:.0f} å­—")

# ========== Query æ“´å±• ==========
class QueryExpander:
    """Query æ“´å±•"""
    
    def __init__(self, config: DoclingRAGConfig):
        self.config = config
    
    def expand_query(self, query: str) -> Dict:
        """æ“´å±•æŸ¥è©¢"""
        
        if not self.config.enable_query_expansion:
            return {
                "original_query": query,
                "keywords": [],
                "expanded_query": query
            }
        
        print(f"\nğŸ” æ“´å±•æŸ¥è©¢ï¼šã€Œ{query}ã€")
        
        try:
            response = client.chat.completions.create(
                model=self.config.query_expansion_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ç‡ˆå“å°ˆå®¶ï¼Œè«‹æ ¹æ“šqueryï¼Œæä¾›å¯èƒ½çš„é«˜åº¦ç›¸é—œé—œéµè©ï¼ˆJSONæ ¼å¼ï¼‰ï¼š{\"keywords\": [\"è©1\", \"è©2\"]}"},
                    {"role": "user", "content": f"æŸ¥è©¢ï¼š{query}\næä¾›æœ€å¤š{self.config.max_keywords}å€‹é—œéµè©"}
                ],
                temperature=0.3,
                max_tokens=200,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            keywords = result.get("keywords", [])
            expanded = f"{query} {' '.join(keywords)}" if keywords else query
            
            print(f"  âœ“ é—œéµè©ï¼š{', '.join(keywords)}")
            
            return {
                "original_query": query,
                "keywords": keywords,
                "expanded_query": expanded
            }
        except Exception as e:
            print(f"  âš ï¸ æ“´å±•å¤±æ•—ï¼š{e}")
            return {
                "original_query": query,
                "keywords": [],
                "expanded_query": query
            }

# ========== æ··åˆè¨­å‚™æª¢ç´¢ï¼ˆä½¿ç”¨é è¨ˆç®— embeddingsï¼‰==========
def search_with_hybrid_device(
    query: str,
    pages: List[PageContent],
    page_embeddings: np.ndarray,  # æ–°å¢ï¼šä½¿ç”¨é è¨ˆç®—çš„ embeddings
    config: DoclingRAGConfig
) -> List[Tuple[PageContent, float]]:
    """
    æ··åˆè¨­å‚™æª¢ç´¢ï¼š
    1. Embedding åˆç¯©ï¼ˆä½¿ç”¨é è¨ˆç®—çš„ embeddingsï¼Œè¶…å¿«ï¼ï¼‰
    2. Reranker MPS ç²¾æ’ï¼ˆå¿«é€Ÿï¼‰
    """
    
    print(f"\nğŸ” æ··åˆè¨­å‚™æª¢ç´¢...")
    print(f"  - ç¸½é æ•¸ï¼š{len(pages)}")
    
    start = time.time()
    
    # ========== éšæ®µ 1: Embedding åˆç¯©ï¼ˆä½¿ç”¨é è¨ˆç®— embeddingsï¼‰==========
    if config.enable_embedding_filter and len(pages) > config.max_embedding_candidates:
        print(f"\n  ğŸ“Š éšæ®µ 1: Embedding åˆç¯©ï¼ˆä½¿ç”¨å¿«å–ï¼Œå‰ {config.max_embedding_candidates} é ï¼‰")
        
        # åªéœ€è¦ç·¨ç¢¼ queryï¼ˆ1 ç§’å…§ï¼ï¼‰
        print(f"    - ç·¨ç¢¼æŸ¥è©¢...")
        query_embedding = embedding_model.encode(
            [query],
            normalize_embeddings=True,
            convert_to_numpy=True
        )[0]
        
        # ä½¿ç”¨é è¨ˆç®—çš„é é¢ embeddings è¨ˆç®—ç›¸ä¼¼åº¦
        print(f"    - ä½¿ç”¨é è¨ˆç®—çš„é é¢ embeddings è¨ˆç®—ç›¸ä¼¼åº¦...")
        similarities = page_embeddings @ query_embedding
        
        # å–å‰ N å€‹å€™é¸
        top_indices = np.argsort(-similarities)[:config.max_embedding_candidates]
        candidate_pages = [pages[i] for i in top_indices]
        
        print(f"    âœ“ ç¯©é¸å‡º {len(candidate_pages)} å€‹å€™é¸é é¢")
        print(f"    - Embedding Top 10 é ç¢¼ï¼š{[pages[i].page_no for i in top_indices[:10]]}")
        
    else:
        candidate_pages = pages
        print(f"  - è·³é embedding åˆç¯©ï¼ˆé æ•¸è¼ƒå°‘ï¼‰")
    
    # ========== éšæ®µ 2: Reranker MPS ç²¾æ’ ==========
    print(f"\n  ğŸ¯ éšæ®µ 2: Reranker ç²¾æ’ ({reranker_device.upper()})ï¼ˆ{len(candidate_pages)} é ï¼‰")
    
    # æ¸…ç† MPS é¡¯å­˜
    if reranker_device == "mps":
        torch.mps.empty_cache()
    
    # æ§‹å»º query-page å°
    pairs = [(query, page.content) for page in candidate_pages]
    
    # Reranker æ‰“åˆ†
    print(f"    - è¨ˆç®—ç²¾ç¢ºç›¸é—œæ€§åˆ†æ•¸...")
    scores = reranker.predict(
        pairs,
        batch_size=config.reranker_batch_size,
        show_progress_bar=True,
        convert_to_numpy=True
    )
    
    # æ¸…ç† MPS é¡¯å­˜
    if reranker_device == "mps":
        torch.mps.empty_cache()
    
    # æ’åº
    page_scores = list(zip(candidate_pages, scores))
    page_scores.sort(key=lambda x: x[1], reverse=True)
    
    # å–å‰ N å€‹
    top_pages = page_scores[:config.max_final_pages]
    
    elapsed = time.time() - start
    
    # é¡¯ç¤ºçµ±è¨ˆ
    print(f"\n  âœ“ æª¢ç´¢å®Œæˆï¼ç¸½è€—æ™‚ï¼š{elapsed:.2f} ç§’")
    print(f"  - é€Ÿåº¦ï¼š{len(pages) / elapsed:.1f} é /ç§’")
    print(f"  - æœ€é«˜åˆ†ï¼š{page_scores[0][1]:.4f}")
    print(f"  - æœ€ä½åˆ†ï¼š{page_scores[-1][1]:.4f}")
    print(f"  - å¹³å‡åˆ†ï¼š{np.mean(scores):.4f}")
    
    # é¡¯ç¤ºå‰å¹¾å
    print(f"\n  ğŸ“Š Top {min(config.show_top_scores, len(top_pages))} é é¢ï¼š")
    for i, (page, score) in enumerate(top_pages[:config.show_top_scores], 1):
        print(f"    {i:2d}. ç¬¬ {page.page_no:3d} é  - {score:.4f}")
    
    return top_pages

# ========== RAG ==========
class RAG:
    """RAG ç”Ÿæˆå™¨"""
    
    def __init__(self, config: DoclingRAGConfig):
        self.config = config
    
    def generate(
        self,
        query: str,
        relevant_pages: List[Tuple[PageContent, float]],
        query_info: Optional[Dict] = None
    ) -> Dict:
        """ç”Ÿæˆå›ç­”"""
        
        if not relevant_pages:
            return {
                "answer": "æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°ç›¸é—œå…§å®¹ã€‚",
                "pages_used": [],
                "tokens_used": 0
            }
        
        print(f"\nğŸ¤– ç”Ÿæˆå›ç­”...")
        
        # æ§‹å»º context
        context = "\n\n".join([
            f"ã€åƒè€ƒè³‡æ–™ {i}ã€‘ï¼ˆPDF ç¬¬ {page.page_no} é ï¼Œç›¸é—œåº¦ï¼š{score:.4f}ï¼‰\n{page.content}"
            for i, (page, score) in enumerate(relevant_pages, 1)
        ])
        
        # Prompt
        messages = [
            {"role": "system", "content": "è«‹æŠŠæˆ‘æä¾›çµ¦ä½ çš„å‹éŒ„å…§å®¹å…§çš„æ‰€æœ‰ç”¢å“è©³ç´°çš„çµ±æ•´å‡ºä¾†ï¼Œä¸èƒ½éºæ¼ã€‚å›ç­”æ™‚å¼•ç”¨é ç¢¼ã€‚"},
            {"role": "user", "content": f"å•é¡Œï¼š{query}\n\nå‹éŒ„å…§å®¹ï¼š\n{context}\n\nè«‹å›ç­”ï¼š"}
        ]
        
        try:
            start = time.time()
            
            response = client.chat.completions.create(
                model=self.config.openai_model,
                messages=messages,
                max_tokens=self.config.max_response_tokens,
                temperature=self.config.temperature
            )
            
            return {
                "answer": response.choices[0].message.content,
                "pages_used": [{"page_no": p.page_no, "score": f"{s:.4f}"} for p, s in relevant_pages],
                "tokens_used": response.usage.total_tokens,
                "time": f"{time.time() - start:.2f}ç§’",
                "query_info": query_info
            }
        except Exception as e:
            return {
                "answer": f"éŒ¯èª¤ï¼š{e}",
                "pages_used": [],
                "tokens_used": 0,
                "error": str(e)
            }

# ========== ä¸»ç³»çµ± ==========
class DoclingRAGSystem:
    """Docling RAG ç³»çµ±ï¼ˆæ··åˆè¨­å‚™ç‰ˆ + é è¨ˆç®— embeddingsï¼‰"""
    
    def __init__(self, config: DoclingRAGConfig):
        self.config = config
        self.parser = DoclingParser(config)
        self.expander = QueryExpander(config)
        self.rag = RAG(config)
        
        self.pages = None
        self.page_embeddings = None  # æ–°å¢ï¼šå„²å­˜é è¨ˆç®—çš„ embeddings
        self.history = []
    
    def initialize(self):
        """åˆå§‹åŒ–"""
        print("="*80)
        print("ğŸš€ åˆå§‹åŒ– Docling RAG ç³»çµ±ï¼ˆæ··åˆè¨­å‚™ç‰ˆ + é è¨ˆç®— embeddingsï¼‰")
        print("="*80)
        
        # è§£æ PDF ä¸¦è¨ˆç®—/è¼‰å…¥ embeddings
        self.pages, self.page_embeddings = self.parser.parse_pdf(self.config.pdf_path)
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦è¨ˆç®— embeddings
        if self.page_embeddings is None:
            print(f"\nâš ï¸ åµæ¸¬åˆ°èˆŠç‰ˆå¿«å–ï¼Œæ­£åœ¨è¨ˆç®— embeddings...")
            self.page_embeddings = self.parser._compute_embeddings(self.pages)
            
            # æ›´æ–°å¿«å–
            print(f"ğŸ’¾ æ›´æ–°å¿«å–ï¼ˆæ–°å¢ embeddingsï¼‰...")
            self.parser._save_cache(
                self.pages, 
                self.config.pdf_path, 
                self.page_embeddings
            )
            print(f"âœ“ å¿«å–å·²æ›´æ–°")
        
        print(f"\nâœ“ ç³»çµ±å°±ç·’ï¼")
        print(f"  - æ¨¡å¼ï¼šEmbedding (CPU) + Reranker ({reranker_device.upper()})")
        print(f"  - Embeddingï¼šBAAI/bge-m3 on CPUï¼ˆé è¨ˆç®—ä¸¦å¿«å–ï¼‰")
        print(f"  - Rerankerï¼šBAAI/bge-reranker-v2-m3 on {reranker_device.upper()}ï¼ˆå¿«é€Ÿï¼‰")
        print(f"  - ç¸½é æ•¸ï¼š{len(self.pages)}")
        print(f"  - Embeddingsï¼šå·²é è¨ˆç®—ä¸¦å¿«å–ï¼ˆæŸ¥è©¢æ™‚åªéœ€ç·¨ç¢¼ queryï¼Œ<1ç§’ï¼‰")
        if self.config.enable_embedding_filter:
            print(f"  - æª¢ç´¢ç­–ç•¥ï¼š{len(self.pages)} é  â†’ Embedding å¿«é€Ÿç¯©é¸ {self.config.max_embedding_candidates} é  â†’ Reranker {reranker_device.upper()} ç²¾æ’ {self.config.max_final_pages} é ")
    
    def query(self, question: str) -> Dict:
        """æŸ¥è©¢"""
        
        print("\n" + "="*80)
        print(f"ğŸ“ {question}")
        print("="*80)
        
        start = time.time()
        
        # Query æ“´å±•
        query_info = self.expander.expand_query(question)
        
        # æ··åˆè¨­å‚™æª¢ç´¢ï¼ˆä½¿ç”¨é è¨ˆç®—çš„ embeddingsï¼‰
        relevant = search_with_hybrid_device(
            query_info["expanded_query"],
            self.pages,
            self.page_embeddings,  # å‚³å…¥é è¨ˆç®—çš„ embeddings
            self.config
        )
        
        # ç”Ÿæˆ
        result = self.rag.generate(question, relevant, query_info)
        result["total_time"] = f"{time.time() - start:.2f}ç§’"
        
        self.history.append(result)
        
        return result

# ========== ä¸»ç¨‹å¼ ==========
def main():
    """ä¸»ç¨‹å¼"""
    
    config = DoclingRAGConfig(
        pdf_path="2025èˆå…‰LED21st(å–®é æ°´å°å¯æœå°‹).pdf",
        
        # Docling è¨­ç½®
        enable_ocr=True,
        enable_table_structure=True,
        keep_temp_files=False,
        
        # Query æ“´å±•
        enable_query_expansion=True,
        
        # Embedding åˆç¯© (CPU)
        enable_embedding_filter=True,
        embedding_model="BAAI/bge-m3",
        max_embedding_candidates=80,
        
        # Reranker ç²¾æ’ (MPS)
        max_final_pages=25,
        reranker_batch_size=8,  # MPS batch size
        show_top_scores=30
    )
    
    system = DoclingRAGSystem(config)
    system.initialize()
    
    print("\nğŸ’¡ è¼¸å…¥å•é¡Œï¼ˆ'q' é›¢é–‹ï¼‰")
    print("-"*80)
    
    while True:
        question = input("\nå•é¡Œ > ").strip()
        
        if question.lower() in ['q', 'quit', 'exit']:
            break
        
        if not question:
            continue
        
        result = system.query(question)
        
        print("\n" + "="*80)
        print("ğŸ“‹ å›ç­”")
        print("="*80)
        print(f"\n{result['answer']}")
        
        print(f"\nğŸ“Š çµ±è¨ˆï¼š")
        print(f"  - ä½¿ç”¨é é¢ï¼š{[p['page_no'] for p in result['pages_used'][:10]]}")
        print(f"  - Tokenï¼š{result['tokens_used']:,}")
        print(f"  - æ™‚é–“ï¼š{result['total_time']}")

if __name__ == "__main__":
    main()